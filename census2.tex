\documentclass{beamer}

%\usepackage[table]{xcolor}
\mode<presentation> {
  \usetheme{Boadilla}
%  \usetheme{Pittsburgh}
%\usefonttheme[2]{sans}
\renewcommand{\familydefault}{cmss}
%\usepackage{lmodern}
%\usepackage[T1]{fontenc}
%\usepackage{palatino}
%\usepackage{cmbright}
  \setbeamercovered{transparent}
\useinnertheme{rectangles}
}
%\usepackage{normalem}{ulem}
%\usepackage{colortbl, textcomp}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{structure}{fg= black}
\definecolor{trial}{cmyk}{1,0,0, 0}
\definecolor{trial2}{cmyk}{0.00,0,1, 0}
\definecolor{darkgreen}{rgb}{0,.4, 0.1}
\usepackage{array}
\beamertemplatesolidbackgroundcolor{white}  \setbeamercolor{alerted
text}{fg=red}

\setbeamertemplate{caption}[numbered]\newcounter{mylastframe}

%\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{colortbl}
%\usepackage[usenames, dvipsnames]{color}
%\setbeamertemplate{caption}[numbered]\newcounter{mylastframe}c
%\newcolumntype{Y}{\columncolor[cmyk]{0, 0, 1, 0}\raggedright}
%\newcolumntype{C}{\columncolor[cmyk]{1, 0, 0, 0}\raggedright}
%\newcolumntype{G}{\columncolor[rgb]{0, 1, 0}\raggedright}
%\newcolumntype{R}{\columncolor[rgb]{1, 0, 0}\raggedright}

%\begin{beamerboxesrounded}[upper=uppercol,lower=lowercol,shadow=true]{Block}
%$A = B$.
%\end{beamerboxesrounded}}
\renewcommand{\familydefault}{cmss}
%\usepackage[all]{xy}

\usepackage{tikz}
\usepackage{lipsum}

 \newenvironment{changemargin}[3]{%
 \begin{list}{}{%
 \setlength{\topsep}{0pt}%
 \setlength{\leftmargin}{#1}%
 \setlength{\rightmargin}{#2}%
 \setlength{\topmargin}{#3}%
 \setlength{\listparindent}{\parindent}%
 \setlength{\itemindent}{\parindent}%
 \setlength{\parsep}{\parskip}%
 }%
\item[]}{\end{list}}
\usetikzlibrary{arrows}
%\usepackage{palatino}
%\usepackage{eulervm}
\usecolortheme{lily}
\newtheorem{com}{Comment}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
 \numberwithin{equation}{section}
%\usepackage[latin1]{inputenc}
\title[Text as Data] % (optional, nur bei langen Titeln n√∂tig)
{Text as Data}

\author{Justin Grimmer}
\institute[Stanford University]{Professor\\Department of Political Science \\Stanford University}
\vspace{0.3in}


\date{September 10th, 2019}%[Big Data Workshop]
%\date{\today}



\begin{document}
\begin{frame}
\titlepage
\end{frame}




\begin{frame}
\frametitle{Texts and Geometry}

Consider a document-term matrix

\begin{eqnarray}
\boldsymbol{X} & = & \begin{pmatrix}
1 & 2 & 0 & \hdots & 0 \\
0 & 0 & 3 & \hdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & 0 & 0 & \hdots & 3 \\
\end{pmatrix}\nonumber
\end{eqnarray}


\pause \invisible<1>{Suppose documents live in a \alert{space}}\pause\invisible<1-2>{ $\leadsto$ rich set of results from linear algebra} \pause
\begin{itemize}
\invisible<1-3>{\item[-] Provides a \alert{geometry}}\pause\invisible<1-4>{$\leadsto$ modify with word weighting} \pause
\invisible<1-5>{\item[-] Natural notions of \alert{distance}} \pause
\invisible<1-6>{\item[-] Building block for clustering, supervised learning, and scaling}
\end{itemize}



\end{frame}



\begin{frame}
\frametitle{Texts in Space}
\pause
\begin{eqnarray}
\invisible<1>{\text{Doc1} & = & (1, 1, 3, \hdots, 5) \nonumber \\ } \pause
\invisible<1-2>{\text{Doc2} & = & (2, 0, 0, \hdots, 1) \nonumber \\} \pause
\invisible<1-3>{\textbf{Doc1}, \textbf{Doc2} & \in & \Re^{J} \nonumber } \pause
\end{eqnarray}



\invisible<1-4>{\alert{Inner Product} between documents: } \pause
\begin{eqnarray}
\invisible<1-5>{\textbf{Doc1} \cdot \textbf{Doc2}  &  = &  (1, 1, 3, \hdots, 5)^{'} (2, 0, 0, \hdots, 1) \nonumber \\} \pause
\invisible<1-6>{  & = &  1 \times 2 + 1 \times 0 + 3 \times 0 + \hdots + 5 \times 1  \nonumber \\} \pause
\invisible<1-7>{   & = & 7 \nonumber}
   \end{eqnarray}

\end{frame}


\begin{frame}
\frametitle{Vector Length}

\begin{columns}[]

\column{0.6\textwidth}
\only<1>{\scalebox{0.5}{\includegraphics{Length1.pdf}}}
\only<2>{\scalebox{0.5}{\includegraphics{Length2.pdf}}}
\only<3>{\scalebox{0.5}{\includegraphics{Length3.pdf}}}
\only<4-5>{\scalebox{0.5}{\includegraphics{Length4.pdf}}}

\column{0.4\textwidth}
\begin{itemize}
\invisible<1>{\item[-] \alert{Pythagorean Theorem}: Side with length $a$}
\invisible<1-2>{\item[-] Side with length $b$ and right triangle}
\invisible<1-3>{\item[-] $c = \sqrt{ a^2 + b^2} $ }
\invisible<1-4>{\item[-] \alert{This is generally true} }
\end{itemize}

\end{columns}

\pause \pause \pause \pause
\end{frame}




\begin{frame}
\frametitle{Vector (Euclidean) Length}

\begin{defn} Suppose $\boldsymbol{v} \in \Re^{J}$. Then, we will define its \alert{length} as
\begin{eqnarray}
||\boldsymbol{v}|| & = & (\boldsymbol{v} \cdot \boldsymbol{v} )^{1/2} \nonumber \\
               & = & (v_{1}^2 + v_{2}^{2} + v_{3}^{2} + \hdots + v_{J}^{2} )^{1/2} \nonumber
\end{eqnarray}
\end{defn}


\end{frame}


\begin{frame}
\frametitle{Measures of Dissimilarity}

Initial guess$\leadsto$ \alert{Distance metrics} \\

Properties of a metric: (distance function) $d(\cdot, \cdot)$.  Consider arbitrary documents $\boldsymbol{X}_{i}$, $\boldsymbol{X}_{j}$, $\boldsymbol{X}_{k}$ \pause
\begin{itemize}
\invisible<1>{\item[1)] $d(\boldsymbol{X}_{i}, \boldsymbol{X}_{j}) \geq 0$} \pause
\invisible<1-2>{\item[2)] $d(\boldsymbol{X}_{i}, \boldsymbol{X}_{j} ) = 0 $ if and only if $\boldsymbol{X}_{i} = \boldsymbol{X}_{j}$} \pause
\invisible<1-3>{\item[3)] $d(\boldsymbol{X}_{i}, \boldsymbol{X}_{j} ) = d(\boldsymbol{X}_{j}, \boldsymbol{X}_{i} )$} \pause
\invisible<1-4>{\item[4)] $d(\boldsymbol{X}_{i}, \boldsymbol{X}_{k}) \leq d(\boldsymbol{X}_{i}, \boldsymbol{X}_{j})  + d(\boldsymbol{X}_{j}, \boldsymbol{X}_{k})$} \pause
\end{itemize}

\vspace{0.5in}

\invisible<1-5>{Explore \alert{distance} functions to compare documents}$\leadsto$\pause\invisible<1-6>{Do we want additional assumptions/properties?}

\end{frame}




\begin{frame}
\frametitle{Measuring the Distance Between Documents}

\alert{Euclidean Distance}


\begin{center}
\only<1>{\scalebox{0.5}{\includegraphics{Doc1.pdf}}}
\only<2>{\scalebox{0.5}{\includegraphics{Doc2.pdf}}}
\only<3>{\scalebox{0.5}{\includegraphics{Doc3.pdf}}}
\end{center}



\end{frame}


\begin{frame}
\frametitle{Measuring the Distance Between Documents}

\begin{defn}
The Euclidean distance between documents $\boldsymbol{X}_{i}$ and $\boldsymbol{X}_{j}$ as

\begin{eqnarray}
||\boldsymbol{X}_{i} - \boldsymbol{X}_{j}||  & =  & \sqrt{\sum_{m=1}^{J} \left(x_{im} -  x_{jm} \right)^2} \nonumber
\end{eqnarray}

\end{defn}

\pause
\invisible<1>{Suppose $\boldsymbol{X}_{i} = (1, 4)$ and $\boldsymbol{X}_{j} = (2, 1)$.  The distance between the documents is:
\begin{eqnarray}
||(1, 4) - (2,1) || & = & \sqrt{ (1 -2 )^2 + (4 - 1)^2 } \nonumber\\
   & = & \sqrt{10}  \nonumber
\end{eqnarray}
}

\end{frame}



\begin{frame}
\frametitle{Measuring Similarity (and removing document length)}



\pause

\invisible<1>{What properties should similarity measure have?} \pause
\begin{itemize}
\invisible<1-2>{\item[-] Maximum: document with itself} \pause
\invisible<1-3>{\item[-] Minimum: documents have no words in common (\alert{orthogonal} ) } \pause
\invisible<1-4>{\item[-] Increasing when \alert{more} of same words used } \pause
\invisible<1-5>{\item[-] \alert{?} $s(a, b)  = s(b,a)$.  }  \pause
\end{itemize}

\invisible<1-6>{How should additional words be treated?}


\end{frame}



\begin{frame}
\frametitle{Measuring Similarity}

\begin{center}
\scalebox{0.35}{\includegraphics{Fig1.pdf}}
\end{center}


Measure 1: Inner product \pause  \\
\begin{eqnarray}
\invisible<1>{(2, 1)^{'} \cdot (1, 4) & = & 6 }   \nonumber
\end{eqnarray}



\end{frame}



\begin{frame}

\begin{center}
\only<1-3>{\scalebox{0.35}{\includegraphics{Fig2.pdf}}}
\only<4>{\scalebox{0.35}{\includegraphics{Fig3.pdf}}}
\end{center}



\invisible<1>{\alert{Problem}(?): length dependent }
\begin{eqnarray}
\invisible<1-2>{(4,2)^{'} (1,4) & = & 12 } \nonumber \\
\invisible<1-3>{a \cdot b & = & ||a|| \times ||b|| \times \cos \theta \nonumber }
\end{eqnarray}

\pause \pause \pause



\end{frame}



\begin{frame}
\frametitle{Cosine Similarity}


\begin{center}
\only<7->{\scalebox{0.35}{\includegraphics{Fig4.pdf}}}
\end{center}


\only<7->{
$\cos \theta$: removes document length from similarity measure\\ \pause
\invisible<1-7>{Projects texts to unit length representation$\leadsto$ onto sphere}
}


\only<1-6>{\begin{eqnarray}
\invisible<1>{\cos \theta & = & \left(\frac{a} {||a||}\right)  \cdot \left(\frac{b} {||b||}  \right) \nonumber \\}
\invisible<1-2>{\frac{(4,2)}{||(4,2) ||} & = & (0.89, 0.45) \nonumber \\}
\invisible<1-3>{\frac{(2,1)}{||(2,1) || } & = & (0.89, 0.45) \nonumber \\}
\invisible<1-4>{\frac{(1,4)} {||(1,4)||}  & = & (0.24, 0.97) \nonumber } \\
\invisible<1-5>{(0.89, 0.45)^{'} (0.24, 0.97) & = & 0.65 \nonumber }
\end{eqnarray}
}

\pause \pause \pause \pause \pause \pause \pause

\end{frame}



\begin{frame}
\frametitle{Weighting Words}

Are all words created equal?  \pause
\begin{itemize}
\invisible<1>{\item[-] Treat all words equally} \pause
\invisible<1-2>{\item[-] \alert{Lots of noise} } \pause
\invisible<1-3>{\item[-] Reweight words} \pause
\begin{itemize}
\invisible<1-4>{\item[-] Accentuate words that are likely to be \alert{informative}} \pause
\invisible<1-5>{\item[-] Make specific assumptions about characteristics of \alert{informative} words} \pause
\end{itemize}
\end{itemize}

\invisible<1-6>{How to generate weights?} \pause
\begin{itemize}
\invisible<1-7>{\item[-] Assumptions about separating words} \pause
\invisible<1-8>{\item[-] Use \alert{training} set to identify separating words (Monroe, Ideology measurement)}
\end{itemize}



\end{frame}


\begin{frame}
\frametitle{Weighting Words: TF-IDF Weighting}

What properties do words need to separate concepts? \pause
\begin{itemize}
\invisible<1>{\item[-] Used frequently} \pause
\invisible<1-2>{\item[-] But not too frequently} \pause
\end{itemize}
\invisible<1-3>{\alert{Ex.} If all statements about OBL contain {\tt Bin Laden} than this contributes nothing to similarity/dissimilarity measures\\} \pause

\invisible<1-4>{\alert{Inverse document frequency}:} \pause
\begin{eqnarray}
\invisible<1-5>{\text{n}_{j} & = & \text{No. documents in which word $j$ occurs} \nonumber \\} \pause
\invisible<1-6>{\text{idf}_{j} & = & \log \frac{N} {n_j} \nonumber  \\ } \pause
\invisible<1-7>{\textbf{idf} & = & (\text{idf}_{1} , \text{idf}_{2}, \hdots, \text{idf}_{J} ) \nonumber }
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Weighting Words: TF-IDF Weighting}


Why $\log$ ? \pause
\begin{itemize}
\invisible<1>{\item[-] Maximum at $n_j$ = 1} \pause
\invisible<1-2>{\item[-] Decreases at rate $\frac{1}{n_j} \Rightarrow$ diminishing ``penalty" for more common use} \pause
\invisible<1-3>{\item[-] Other functional forms are fine, embed assumptions about penalization of common use}
\end{itemize}



\end{frame}


\begin{frame}
\frametitle{Weighting Words: TF-IDF}

\pause
\begin{eqnarray}
\invisible<1>{\textbf{X}_{i, \text{idf}}  \equiv \underbrace {\textbf{X}_{i}}_{\text{tf} } \times \textbf{idf} & = & (X_{i1} \times \text{idf}_1 , X_{i2} \times \text{idf}_2 , \hdots, X_{iJ} \times \text{idf}_J) \nonumber \\} \pause
\invisible<1-2>{\textbf{X}_{j,\text{idf}}\equiv \textbf{X}_{j} \times \textbf{idf} & = & (X_{j1} \times \text{idf}_1 , X_{j2} \times \text{idf}_2 , \hdots, X_{jJ} \times \text{idf}_J ) \nonumber} \pause
\end{eqnarray}

\invisible<1-3>{How Does This Matter For Measuring Similarity/Dissimilarity? \\} \pause

\invisible<1-4>{\alert{Inner Product} } \pause
\begin{eqnarray}
\invisible<1-5>{\textbf{X}_{i, \text{idf}} \cdot \textbf{X}_{j, \text{idf}} & = &(\textbf{X}_{i} \times \textbf{idf} )^{'} ( \textbf{X}_{j} \times \textbf{idf})  \nonumber \\} \pause
\invisible<1-6>{ & = & (\text{idf}_1^2 \times  X_{i1} \times X_{j1}) + (\text{idf}^{2}_2 \times X_{i2} \times X_{j2}) + \nonumber \\
 & &  \hdots + (\text{idf}_J^{2} \times X_{iJ} \times X_{jJ}) \nonumber }
 \end{eqnarray}


\end{frame}




\begin{frame}
\frametitle{Weighting Words: Inner Product}

Define: \pause \\
\vspace{0.25in}


\invisible<1>{$\boldsymbol{\Sigma}  = \begin{pmatrix}
                      \text{idf}_1^{2} & 0 & 0 &\hdots & 0 \\
                      0 & \text{idf}_2^{2} & 0 &\hdots &0 \\
                      \vdots & \vdots & \vdots & \ddots & \vdots \\
                      0 & 0 & 0 & \hdots & \text{idf}_J^{2}
                      \end{pmatrix} $} \pause

\vspace{0.25in}
\invisible<1-2>{If we use tf-idf for our documents, then  } \pause
\begin{eqnarray}
\invisible<1-3>{d_{2}(\boldsymbol{X}_{i}, \boldsymbol{X}_{j} ) & = & \sqrt{\sum_{m=1}^{J}(x_{im, \text{idf}} - x_{jm, \text{idf}} )^{2} } \nonumber \\
 & = & \sqrt{(\boldsymbol{X}_{i}  - \boldsymbol{X}_{j})^{'}\boldsymbol{\Sigma} (\boldsymbol{X}_{i}  - \boldsymbol{X}_{j})  } \nonumber }
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Final Product}

Applying some measure of distance, similarity (if symmetric) yields:

$\textbf{D} = \begin{pmatrix}
0 & d (1, 2)  & d(1, 3) & \hdots & d(1, N) \\
\alert{d(2,1)} & 0 &  d(2,3)  & \hdots & d(2, N) \\
\alert{d(3,1)} & \alert{d(3,2)} & 0 & \hdots & d(3, N ) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\alert{d(N,1)} & \alert{d(N,2)} & \alert{d(N,3)}   & \alert{\hdots}  & 0
\end{pmatrix} $


\vspace{0.5in}

\alert{Lower Triangle} contains unique information $N(N-1)/2$


\end{frame}


\begin{frame}

\huge

Learning relationships to classify documents


\end{frame}





\begin{frame}
\frametitle{Types of Classification Problems}


\alert{Topic}: What is this text about? \pause
\invisible<1>{\begin{itemize}
\item[-] Policy area of legislation  \\
$\Rightarrow$ $\{$Agriculture, Crime, Environment, ...$\}$
\item[-] Campaign agendas \\
$\Rightarrow$ $\{$Abortion, Campaign, Finance, Taxing, ...       $\}$
\end{itemize}} \pause

\invisible<1-2>{\alert{Sentiment}: What is said in this text? [\alert{Public Opinion}] } \pause
\invisible<1-3>{\begin{itemize}
\item[-] Positions on legislation\\
 $\Rightarrow$ $\{$ Support, Ambiguous, Oppose $\}$
\item[-] Positions on Court Cases \\
$\Rightarrow$ $\{$ Agree with Court, Disagree with Court $\}$
\item[-] Liberal/Conservative Blog Posts \\
$\Rightarrow$ $\{$ Liberal, Middle, Conservative, No Ideology Expressed $\}$
\end{itemize} } \pause

\invisible<1-4>{\alert{Style}/\alert{Tone}: How is it said?} \pause
\invisible<1-5>{\begin{itemize}
\item[-] Taunting in floor statements\\
 $\Rightarrow$ $\{$ Partisan Taunt, Intra party taunt, Agency taunt, ... $\}$
\item[-] Negative campaigning \\
$\Rightarrow$ $\{$ Negative ad, Positive ad$\}$
\end{itemize} }

\end{frame}





\begin{frame}
\frametitle{Regression models}

Suppose we have $N$ documents, with each document $i$ having label $y_{i} \in \{-1, 1\}\leadsto\{$not, credit claiming$\}$ \pause \\
\invisible<1>{We represent each document $i$ is $\boldsymbol{x}_{i} = (x_{i1}, x_{i2}, \hdots, x_{iJ})$. } \pause  \\

\begin{eqnarray}
\invisible<1-2>{f(\boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y})  & = & \sum_{i=1}^{N}\left( y_{i} - \boldsymbol{\beta}^{'} \boldsymbol{x}_{i} \right)^{2}  \nonumber \\} \pause
\invisible<1-3>{\widehat{\boldsymbol{\beta} } & = & \text{arg min}_{\boldsymbol{\beta}} \left\{\sum_{i=1}^{N}\left( y_{i} - \boldsymbol{\beta}^{'} \boldsymbol{x}_{i} \right)^{2}\right\} \nonumber \\} \pause
 \invisible<1-4>{& = & \left( \boldsymbol{X}^{'}\boldsymbol{X}   \right)^{-1}\boldsymbol{X}^{'}\boldsymbol{Y} \nonumber } \pause
\end{eqnarray}

\invisible<1-5>{Problem: \\} \pause
\begin{itemize}
\invisible<1-6>{\item[-] $J$ will likely be large (perhaps $J> N$)} \pause
\invisible<1-7>{\item[-] There many correlated variables} \pause
\end{itemize}

\invisible<1-8>{Predictions will be \alert{variable}}


\end{frame}


\begin{frame}
\frametitle{Mean Square Error}
Suppose $\theta$ is some value of the true parameter \pause \\
\invisible<1>{Bias: \\} \pause
\begin{eqnarray}
\invisible<1-2>{\text{Bias} & = & E[\widehat{\theta} - \theta]\nonumber } \pause
\end{eqnarray}

\invisible<1-3>{We may care about average distance from truth} \pause

\begin{eqnarray}
\invisible<1-4>{\text{E}[(\hat{\theta} - \theta)^{2}]}\pause\invisible<1-5>{ & = & E[\hat{\theta}^{2}]  - 2 \theta E[\hat{\theta}] + \theta^2 } \pause \nonumber \\
 \invisible<1-6>{& = &  E[\hat{\theta}^{2}] - E[\hat{\theta}]^{2} + E[\hat{\theta}]^{2}- 2 \theta E[\hat{\theta}] + \theta^2} \pause  \nonumber \\
\invisible<1-7>{& = & E[\hat{\theta}^{2}] - E[\hat{\theta}]^{2} +  (E[\widehat{\theta} - \theta])^2 } \pause \nonumber \\
  \invisible<1-8>{& = & \text{Var}(\hat{\theta}) + \text{Bias}^{2} } \pause \nonumber
\end{eqnarray}

\invisible<1-9>{To reduce MSE, we are willing to induce bias to decrease variance$\leadsto$ methods that \alert{shrink} coefficeints toward zero}

\end{frame}


\begin{frame}
\frametitle{Ridge Regression}

Penalty for model complexity \pause

\begin{eqnarray}
\invisible<1>{f(\boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y} ) }\pause \invisible<1-2>{& = & \sum_{i=1}^{N} \left(y_{i} - \
\beta_{0} - \sum_{j=1}^{J}\beta_{j} x_{ij}\right)^{2} } \pause \invisible<1-3>{ + \underbrace{\lambda \sum_{j=1}^{J} \beta_{j}^{2}}_{\text{Penalty}} } \pause \nonumber
\end{eqnarray}

\invisible<1-4>{where:} \pause

\begin{itemize}
\invisible<1-5>{\item[-] $\beta_{0}\leadsto$ intercept} \pause
\invisible<1-6>{\item[-] $\lambda\leadsto$ penalty parameter} \pause
\invisible<1-7>{\item[-] Standardized $\boldsymbol{X}$ (coefficients on same scale)}
\end{itemize}


\end{frame}

\begin{frame}
\frametitle{Ridge Regression$\leadsto$ Optimization}

\begin{eqnarray}
\boldsymbol{\beta}^{\text{Ridge}} & = & \text{arg min}_{\boldsymbol{\beta}} \left\{f(\boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y})\right\} \nonumber  \pause \\
\invisible<1>{& = & \text{arg min}_{\boldsymbol{\beta}} \left\{\sum_{i=1}^{N} \left(y_{i} - \beta_{0} - \sum_{j=1}^{J}\beta_{j} x_{ij}\right)^{2}  + \lambda \sum_{j=1}^{J} \beta_{j}^{2}\right\} } \pause \nonumber \\
 \invisible<1-2>{& = & \text{arg min}_{\boldsymbol{\beta}} \left\{ (\boldsymbol{Y} - \boldsymbol{X}^{'} \boldsymbol{\beta})^{'}(\boldsymbol{Y} - \boldsymbol{X}^{'} \boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^{'}\boldsymbol{\beta} \right\} } \nonumber \\
\invisible<1-3>{& = & \left(\boldsymbol{X}^{'}\boldsymbol{X} + \lambda \boldsymbol{I}_{J}     \right)^{-1} \boldsymbol{X}^{'} \boldsymbol{Y} } \nonumber
\end{eqnarray}

\invisible<1-2>{Demean the data and set $\beta_{0} = \bar{y} = \sum_{i=1}^{N} \frac{y_{i}}{N}$ }
\pause \pause
\end{frame}



\begin{frame}
\frametitle{Lasso Regression Objective Function}

Different Penalty for Model Complexity

\begin{eqnarray}
f(\boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y} ) & = & \sum_{i=1}^{N} \left(y_{i} - \beta_{0} - \sum_{j=1}^{J} \beta_{j} x_{ij}  \right)^{2} + \lambda \sum_{j=1}^{J} \underbrace{|\beta_{j}|}_{\text{Penalty}} \nonumber \pause
\end{eqnarray}


\end{frame}



\begin{frame}
\frametitle{Comparing Ridge and LASSO}


\only<1>{\scalebox{0.8}{\includegraphics{RidgeExamp1.pdf}}}
\only<2>{\scalebox{0.8}{\includegraphics{LassoExamp1.pdf}}}


\end{frame}

\begin{frame}
\frametitle{Comparing Ridge and LASSO}

Contrast $\beta = (\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}} )$ and $\tilde{\beta} = (1, 0)$ \pause

\invisible<1>{Under ridge:}\pause
\begin{eqnarray}
\invisible<1-2>{\sum_{j=1}^{2} \beta_{j}^{2} & = & \frac{1}{2} + \frac{1}{2} = 1\nonumber \\} \pause
\invisible<1-3>{\sum_{j=1}^{2} \tilde{\beta}_{j}^{2}  & = &  1 + 0 = 1 } \pause \nonumber
\end{eqnarray}

\invisible<1-4>{Under LASSO } \pause
\begin{eqnarray}
\invisible<1-5>{\sum_{j=1}^{2} |\beta_{j}| & = & \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{2}}  = \sqrt{2} \nonumber \\} \pause
\invisible<1-6>{\sum_{j=1}^{2} |\tilde{\beta}_{j}| & = & 1 +0 = 1 \nonumber }
\end{eqnarray}

\end{frame}


\begin{frame}
\frametitle{Ridge and LASSO: The Elastic-Net}

Combining the two criteria $\leadsto$ Elastic-Net

\begin{small}
\begin{eqnarray}
f(\boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y} ) & = & \frac{1}{2N} \sum_{i=1}^{N}\left(y_{i} - \beta_{0} - \sum_{j=1}^{J} \beta_{j} x_{ij} \right)^2 + \lambda \sum_{j=1}^{J} \left(\frac{1}{2} (1-\alpha)\beta_{j}^2 + \alpha|\beta_{j}|    \right) \nonumber
\end{eqnarray}
\end{small}


\end{frame}




\begin{frame}
\frametitle{Selecting $\lambda$}

How do we determine $\lambda$? $\leadsto$ Cross validation  \pause \\
\invisible<1>{Applying models gives score (probability) of document belong to class$\leadsto$ threshold to classify} \pause \\


\end{frame}


\begin{frame}
\frametitle{Cross-Validation: Some Intuition}

Optimal division of data for prediction: \pause
\begin{itemize}
\invisible<1>{\item[-] Train: build model} \pause
\invisible<1-2>{\item[-] Validation: assess model} \pause
\invisible<1-3>{\item[-] Test: predict remaining data} \pause
\end{itemize}

\invisible<1-4>{K-fold Cross-validation idea: create many training and test sets.  } \pause
\begin{itemize}
\invisible<1-5>{\item[-] Idea: use observations both in training and test sets} \pause
\invisible<1-6>{\item[-] Each step: use held out data to evaluate performance} \pause
\invisible<1-7>{\item[-] \alert{Avoid overfitting} and have context specific penalty } \pause
\end{itemize}

\invisible<1-8>{Estimates:}
\begin{eqnarray}
\invisible<1-8>{\text{Error} & = & \text{E}\left[\text{E}[L(\boldsymbol{Y} , f(\hat{\boldsymbol{\beta}} , \boldsymbol{X}))| \mathcal{T} ] \right] \nonumber }
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Cross-Validation: A How To Guide}

Process: \pause
\begin{itemize}
\invisible<1>{\item[-]  Randomly partition data into K groups. } \pause
\invisible<1-2>{\item[] (Group 1, Group 2, Group3, $\hdots$, Group K ) } \pause
\invisible<1-3>{\item[-]  Rotate through groups as follows} \pause
\end{itemize}
\begin{tabular}{lll}
\invisible<1-4>{Step & Training & Validation (``Test") \\} \pause
\invisible<1-5>{1 & Group2, Group3, Group 4, $\hdots$, Group K & Group 1\\} \pause
\invisible<1-6>{2 & Group 1, Group3, Group 4, $\hdots$, Group K & Group 2 \\} \pause
\invisible<1-7>{3 & Group 1, Group 2, Group 4, $\hdots$, Group K & Group 3 \\} \pause
\invisible<1-8>{$\vdots$ & $\vdots$ & $\vdots$ \\} \pause
\invisible<1-9>{K & Group 1, Group 2, Group 3, $\hdots$, Group K - 1 & Group K }
\end{tabular}


\end{frame}

\begin{frame}
\frametitle{Cross-Validation: A How To Guide}
\footnotesize
\begin{tabular}{lll}
Step & Training & Validation (``Test") \\
1 & Group2, Group3, Group 4, $\hdots$, Group K & Group 1\\
2 & Group 1, Group3, Group 4, $\hdots$, Group K & Group 2 \\
3 & Group 1, Group 2, Group 4, $\hdots$, Group K & Group 3 \\
$\vdots$ & $\vdots$ & $\vdots$ \\
K & Group 1, Group 2, Group 3, $\hdots$, Group K - 1 & Group K
\end{tabular}
\normalsize
 \pause \invisible<1>{Strategy: } \pause
\begin{itemize}
\invisible<1-2>{\item[-] Divide data into $K$ groups} \pause
\invisible<1-3>{\item[-] Train data on $K-1$ groups.  Estimate $\hat{f}^{-K}(\boldsymbol{\beta}, \boldsymbol{X})$  } \pause
\invisible<1-4>{\item[-] Predict values for $K^{\text{th}}$} \pause
\invisible<1-5>{\item[-] Summarize performance with loss function: $L(\boldsymbol{Y}_i , \hat{f}^{-k} (\boldsymbol{\beta}, \boldsymbol{X})  ) $} \pause
\begin{itemize}
\invisible<1-6>{\item[-] Mean square error, Absolute error, Prediction error, ...} \pause
\end{itemize}
\invisible<1-7>{\item[] CV(ind. classification)  = $ \frac{1}{N}\sum_{i=1}^{N} L(\boldsymbol{Y}_i , f^{-k} (\boldsymbol{\beta}, \boldsymbol{X}_i)  ) $} \pause
\invisible<1-8>{\item[] CV(proportions)   =  $\frac{1}{K} \sum_{j=1}^{K} \text{Mean Square Error Proportions from Group j}$} \pause
\invisible<1-9>{\item[-] Final choice: model with highest $CV$ score}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{How Do We Select $K$? (HTF, Section 7.10)  }

Common values of $K$
\begin{itemize}
\item[-] $K = 5$: Five fold cross validation
\item[-] $K = 10$: Ten fold cross validation
\item[-] $K = N $: Leave one out cross validation
\end{itemize}

Considerations:
\begin{itemize}
\item[-] How sensitive are inferences to number of coded documents? (HTF, pg 243-244)
\item[-] 200 labeled documents
\begin{itemize}
\item[-] $K= N \rightarrow$ 199 documents to train,
\item[-] $K = 10 \rightarrow$ 180 documents to train
\item[-] $K = 5 \rightarrow$ 160 documents to train
\end{itemize}
\item[-] 50 labeled documents
\begin{itemize}
\item[-] $K= N \rightarrow$ 49 documents to train,
\item[-] $K = 10 \rightarrow$ 45 documents to train
\item[-] $K = 5 \rightarrow$ 40 documents to train
\end{itemize}
\item[-] How long will it take to run models?
\begin{itemize}
\item[-] $K-$fold cross validation requires $K \times $ One model run
\end{itemize}
\item[-] What is the correct loss function?
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{If you cross validate, you really need to cross validate (Section 7.10.2, ESL)}

\begin{itemize}
\item[-] Use CV to estimate prediction error
\item[-] \alert{All} supervised steps performed in cross-validation
\item[-] \alert{Underestimate} prediction error
\item[-] \alert{Could lead to selecting lower performing model}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Example from Facebook Data}

What do people say to legislators?  (Franco, Grimmer, and Lee 2017)
\begin{itemize}
\item[1)] Example: estimating classification error
\begin{itemize}
\item[a)] Accuracy in legislator posts: 75\%
\item[b)] Accuracy in public posts: 66.25\%
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Credit Claiming (Back to Ridge/Lasso, Grimmer, Westwood, and Messing 2014)}

\begin{footnotesize}
%\#\#credit is a 797 element long binary vector

%\#\#dtm is a 797 x 7587 document term matrix
\begin{semiverbatim}
\only<1>{library(glmnet)

set.seed(8675309) \#\#setting seed

folds<- sample(1:10, nrow(dtm), replace=T) \#\#assigning to fold

out\_of\_samp<- c()  \#\#collecting the predictions}


\only<2>{for(z in 1:10)\{

  train<- which(folds!=z) \#\#the observations we will use to train the model

  test<- which(folds==z) \#\#the observations we will use to test the model

  part1<- cv.glmnet(x = dtm[train,], y = credit[train], alpha = 1, family = \'binomial\') \#\#fitting the LASSO model on the data.

  \#\# alpha = 1 -> LASSO

  \#\# alpha = 0 -> RIDGE

  \#\# 0<alpha<1 -> Elastic-Net

  out\_of\_samp[test]<- predict(part1, newx= dtm[test,], s = part1\$lambda.min, type =\'class\') \#\#predicting the labels

  print(z) \#\#printing the labels

  \}

conf\_table<- table(out\_of\_samp, credit)  \#\#calculating the confusion table

> round(sum(diag(conf\_table))/len(credit), 3)

[1] \alert{0.844}
}
\end{semiverbatim}
\end{footnotesize}
\end{frame}





\begin{frame}
\frametitle{Ensemble Learning: Intuition}

\alert{Heuristic} (upon which we'll improve):\pause\invisible<1>{ if regressions are \alert{accurate} and \alert{diverse}$\rightarrow$ ensemble methods improve} \pause \\
\invisible<1-2>{\alert{Intuition}: } \pause
\begin{itemize}
\invisible<1-3>{\item[-] Classify observations into two categories (Category 1, Category 2). }  \pause
\invisible<1-4>{\item[-] True labels: evenly distributed across two categories}  \pause
\invisible<1-5>{\item[-] Three classifiers with $75\%$ accuracy, but independent  }\pause
\invisible<1-6>{\item[-] Implement majority voting rule  }\pause
\end{itemize}
\begin{eqnarray}
\invisible<1-7>{\text{Pr(Correct Guess}| \text{Votes} )} \pause \invisible<1-8>{ & = & \text{Pr(3 correct)} + \text{Pr(2 correct)} } \pause \nonumber \\
 \invisible<1-9>{& = & 0.75^3 + 3 \times (0.75^2 \times 0.25)} \pause  \nonumber \\
  \invisible<1-10>{& = &  0.844 \nonumber }
  \end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{Ensemble Learning: Intuition}



\only<1>{\scalebox{0.45}{\includegraphics{Ensemble1.pdf}}}






\end{frame}


\begin{frame}
\frametitle{Ensemble Learning: Intuition}

\alert{Diverse} and \alert{Accurate} matter.

\only<1>{\scalebox{0.45}{\includegraphics{Ensemble2.pdf}}}
\only<2>{\scalebox{0.45}{\includegraphics{Ensemble3.pdf}}}


\end{frame}



\begin{frame}
\frametitle{Wisdom of the Crowds:}

Goal: estimate an observation's category$\leadsto$ $Y \in \{0, 1\}$ \pause \\
\invisible<1>{Classifiers: (suppose) a sequence of identically distributed (\alert{not necessarily independent}) random variables. } \pause  \\
\invisible<1-2>{Suppose $Y = 1$} \pause \\
\invisible<1-3>{Guess from classifer $m$ is $B_{m}$ with Pr$(B_{i} = 1) = p>0.5$.  \\} \pause
\begin{eqnarray}
\invisible<1-4>{\bar{B} & = & \sum_{m=1}^{M} \frac{B_{m}}{M} \nonumber } \pause
\end{eqnarray}

\invisible<1-5>{\alert{Wisdom of crows} (Condorcet Jury Theorem)} \pause
\begin{eqnarray}
\invisible<1-6>{\lim_{M\rightarrow \infty} P(\bar{B}> 0.5) & = & 1 \nonumber }
\end{eqnarray}

\end{frame}


\begin{frame}
\frametitle{Wisdom of the Crowds}

Suppose $B_{m}$ have variance $\sigma^2$ and pairwise correlation $\rho$. \pause   \\
\invisible<1>{Then, } \pause

\begin{eqnarray}
\invisible<1-2>{\text{var}(\bar{B}) }\pause\invisible<1-3>{& = & \text{var}\left(\sum_{i=1}^{M} \frac{B_{i}}{M}  \right)} \pause  \nonumber \\
 \invisible<1-4>{& = & \frac{1}{M^{2}} \sum_{i=1}^{M} \text{var}\left(B_{i}\right) + \frac{2}{M^2} \sum_{i<j} \text{cov}(B_{i}, B_{j}) \nonumber } \pause\\
 \invisible<1-5>{& = & \frac{M \sigma^2}{M^2}  + \frac{2}{M^2} \rho \sigma^2 {{M}\choose{2}} \nonumber } \pause\\
\invisible<1-6>{& = & \underbrace{\rho \sigma^2}_{\text{Resolve with independence}} + \underbrace{\frac{1 - \rho}{M} \sigma^2}_{\text{Resolve with $\uparrow$classifiers}} \nonumber }
\end{eqnarray}


\end{frame}


\begin{frame}
\frametitle{\alert{Bagging}: bootstrap aggregation}

\begin{small}
Creating Weak Classifiers with resampling: \pause
\begin{itemize}
\invisible<1>{\item[-] Suppose we have dependent variables $\boldsymbol{Y}$ and data $\boldsymbol{X}$} \pause
\invisible<1-2>{\item[-] For each bootstrap step $m$, $(m = 1,2, \hdots, M)$ draw $N$ observations with replacement, $\tilde{\boldsymbol{Y}}_{m}$, $\tilde{\boldsymbol{X}}_{m}$.  } \pause
\invisible<1-3>{\item[-] Train classifier on bootstrapped data, } \pause
\begin{eqnarray}
\invisible<1-4>{\tilde{\boldsymbol{Y}}_{m} & = & f^{m}(\tilde{\boldsymbol{X}}_{m}, \widehat{\boldsymbol{\beta}}, \boldsymbol{\lambda} ) \nonumber } \pause \\
\invisible<1-5>{\hat{f}^{m}(\boldsymbol{x}_{i} , \widehat{\boldsymbol{\beta}}, \boldsymbol{\lambda} ) &= & \text{Classifier from m$^{\text{th}}$ iteration at } \boldsymbol{x}_{i}}  \nonumber \pause
\end{eqnarray}
\invisible<1-6>{\item[-] Aggregating across classifiers, } \pause
\begin{eqnarray}
\invisible<1-7>{f_{\text{bag}}(\boldsymbol{x}_{i}) & = & \frac{1}{M} \sum_{m=1}^{M} \hat{f}^{m}(\boldsymbol{x}_{i} , \widehat{\boldsymbol{\beta}}, \boldsymbol{\lambda}) \nonumber } \pause
\end{eqnarray}
\invisible<1-8>{\item[-] Only leads to a difference in estimate if classifiers are non-linear. } \pause
\invisible<1-9>{\item[-] Strong Correlation between classifiers (recall optimal division from previous slide)}
\end{itemize}
\end{small}
\end{frame}


\begin{frame}
\frametitle{Classification and Regression Trees (CART): Intuition}

Consider regression $E[Y|\boldsymbol{x}_{i}]$. \pause  \\
\invisible<1>{With no assumptions, \alert{stratify}$\leadsto$ different mean for unique values of $\boldsymbol{x}_{i}$\\} \pause
\begin{itemize}
\invisible<1-2>{\item[-] Within each strata $p$, compute average $Y$} \pause
\begin{eqnarray}
\invisible<1-3>{\bar{Y}|\boldsymbol{x}_{p} & = & \sum_{i=1}^{N} \frac{I(\boldsymbol{x}_{i} = \boldsymbol{x}_{p})Y_{i}}{\sum_{t=1}^{N} I(\boldsymbol{x}_{t} = \boldsymbol{x}_{p}) } \nonumber } \pause
\end{eqnarray}
\end{itemize}

\invisible<1-4>{Implies that for test data we would fit:} \pause


\begin{eqnarray}
\invisible<1-5>{\hat{f}(\boldsymbol{x}_{i}) & = & \sum_{p=1}^{P} \bar{Y}|\boldsymbol{x}_{p}  I(\boldsymbol{x}_{i} = \boldsymbol{x}_{p}) \nonumber }\pause\\
\invisible<1-6>{& = & \sum_{p=1}^{P} c_{p}  I(\boldsymbol{x}_{i} = \boldsymbol{x}_{p}) \nonumber }\pause
\end{eqnarray}

\invisible<1-7>{Curse of dimensionality(!!!)\\} \pause
\invisible<1-8>{Approximate with \alert{regions}$\leadsto$ search for splits of data to approximate stratification}







\end{frame}




\begin{frame}
\frametitle{Classification and Regression Trees (CART): Objective function}

Labels $\boldsymbol{Y}_{i}$ and documents $\boldsymbol{x}_{i}$

\begin{eqnarray}
E[Y| \boldsymbol{x}_{i}] & = & \widehat{f}(\boldsymbol{x}_{i}) \nonumber \\
& = & \sum_{p=1}^{P} c_{p} I (\boldsymbol{x}_{i} \in R_{p}) \nonumber
\end{eqnarray}

where:
\begin{itemize}
\item[-] $R_{p}$ describes a \alert{region} $\leadsto$ node
\item[-] $c_{p}$ describes values of $Y_{i}$ for document in $R_{p}$
\end{itemize}


\end{frame}


\begin{frame}
\frametitle{Classification and Regression Trees (CART): Optimization function}

Suppose we want to minimize sum of squared residuals with each \alert{node}\\ \pause

\invisible<1>{Then $c_{p} = $ Average $Y$ for documents assigned to $R_{p}$ \\} \pause
\begin{eqnarray}
\invisible<1-2>{\widehat{c}_{p} & =&  \sum_{i=1}^{N} \frac{Y_{i} I(\boldsymbol{x}_{i} \in R_{p} )  }{\sum_{j=1}^{N} I(\boldsymbol{x}_{j} \in R_{p} )  } \nonumber } \pause
\end{eqnarray}

\invisible<1-3>{Determining an optimal partition$\leadsto$ NP-Hard.  \\} \pause
\invisible<1-4>{Suppose we are in some node (perhaps at the start).  \\} \pause
\invisible<1-5>{Greedy algorithm:} \pause
\begin{footnotesize}
\begin{eqnarray}
\invisible<1-6>{(j^{*}, s^{*})  & = & \text{arg min}_{j, s} \left[ \underbrace{\text{min}_{c_{1}} \sum_{i=1}^{N}I(x_{ij}< s)(Y_{i} - c_{1})^2}_{\text{``cost" group 1}}  + \underbrace{\text{min}_{c_{2}} \sum_{i=1}^{N}I(x_{ij}> s)(Y_{i} - c_{2})^2}_{\text{``cost" group 2}}   \right] \nonumber }
\end{eqnarray}
\end{footnotesize}


\end{frame}


\begin{frame}
\frametitle{Classification and Regression Trees (CART): Algorithm}

\begin{itemize}
\item[-] Start in Node
\item[-] Partition according to Greedy algorithm
\item[-] Continue until some stopping rule: number of observations per node
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{CART Picture (Spirling 2008)}

\scalebox{0.7}{\includegraphics{CART_Example.png}}

\end{frame}


\begin{frame}
\frametitle{Forests and Trees}

Recall: accurate (unbiased) and uncorrelated classifiers \pause
\begin{itemize}
\invisible<1>{\item[-] Grow trees deeply$\leadsto$ unbiased classifers, though high variance} \pause
\invisible<1-2>{\item[-] \alert{Average}$\leadsto$ reduces variance, but will be correlated} \pause
\invisible<1-3>{\item[-] Random forest$\leadsto$ introduce additional sampling to induce independence$\leadsto$ Only split on subset of variables}
\end{itemize}


\end{frame}


\begin{frame}
\frametitle{Random Forest Algorithm (ESL, 588)}

\pause
\begin{itemize}
\invisible<1>{\item[1)] For $m$ bootstrap samples $(m = 1,\hdots, M)$, draw $N$ observations with replacement, $\tilde{\boldsymbol{Y}_{m}}, \tilde{\boldsymbol{X}_{m}}$} \pause
\invisible<1-2>{\item[2)] Until a minimum node size is reached:} \pause
\begin{itemize}
\invisible<1-3>{\item[i)] \alert{Select $z$ of the $J$ variables}$\leadsto$ introduces independences across the trees} \pause
\invisible<1-4>{\item[ii)] Among those $z$, select the best split node} \pause
\invisible<1-5>{\item[iii)] Split into daughter nodes} \pause
\end{itemize}
\invisible<1-6>{\item[3)] The result is an ensemble (forest) of trees $\boldsymbol{T} = (T_{1}, T_{2}, \hdots, T_{M})$, } \pause
\begin{eqnarray}
\invisible<1-7>{\hat{f}(\boldsymbol{x}_{i}) & = & \frac{1}{M} \sum_{m=1}^{M} T_{m} (\boldsymbol{x}_{i}) \nonumber } \pause
\end{eqnarray}
\end{itemize}

\invisible<1-8>{{\tt RandomForest}$\leadsto$ Not a silver bullet!} \pause
\begin{itemize}
\invisible<1-9>{\item[-] With many poor predictors$\leadsto$ the $p$ selected may be meaningless} \pause
\invisible<1-10>{\item[-] Wager and Athey (2015): Random Forest for estimating heterogeneous effects}
\end{itemize}


\end{frame}









% \begin{frame}
% \frametitle{Common Ensemble Methods}

% \alert{Boosting}: sequential training of weak classifiers
% \begin{itemize}
% \item[-] Method for combining several \alert{weak} classifiers
% \item[-] Basic idea:
% \begin{itemize}
% \item[-] Model 1: classify initially based on all data (equal weight)
% \item[-] Model 2: classify all data, more weight to incorrectly classified data
% \item[-] Model 3: classify all data, more weight to incorrectly classified data
% \item[] $\hdots $
% \item[-] Model M: classify all data, more weight to incorrectly classified data
% \end{itemize}
% \item[-] Aggregate using weighted committee
% \end{itemize}



% \end{frame}






\begin{frame}
\frametitle{Super Learning}


\begin{itemize}
\item[1) ] Set of hand labeled documents.  For each $i$, $(i=1, \hdots, N_{\text{train}})$
\begin{itemize}
\invisible<1>{\item[] $Y_{i, \text{train} } \in \{C_{1} C_{2}, \hdots C_{K} \}$}
\end{itemize}
\invisible<1-2>{\item[2)] Estimate relationship between labels and words }
\begin{itemize}
\invisible<1-3>{\item[-] Each document $i$  is a \alert{count vector} of $K$ words}
\invisible<1-4>{\item[] $ \textbf{x}_{i, \text{train}}  =  (X_{i1}, X_{i2}, \hdots, X_{iK}  )$ } \nonumber
\end{itemize}
\end{itemize}

\only<1->{
\begin{eqnarray}
\invisible<1-5>{\text{Pr}(Y_{i} = C_{k} | \textbf{x}_{i} )\invisible<1-6>{_{\text{train}}}\invisible<1-7>{& = & \widehat{g} (\textbf{x}_{i} )_{\text{train}} } \nonumber }
\end{eqnarray}
}
%\only<7->{
%\begin{eqnarray}
%\text{Pr}(Y_{i}  = \text{Credit} | \textbf{w}_{i} )_{\text{train}} \invisible<1-7>{& = & \widehat{g} (\textbf{w}_{i} )_{\text{train}} } %\nonumber
%\end{eqnarray}
%}

\begin{itemize}
\invisible<1-7>{\item[-] Identify systematic relationship between words, labels}  \invisible<1-8>{$\leadsto$ Data and \alert{assumptions}}
\begin{itemize}
\invisible<1-9>{\item[-] LASSO (Tibshirani 1996): \alert{sparsity} }
\invisible<1-10>{\item[-] KRLS (Hainmueller and Hazlett 2013): \alert{dense}, flexible surface}
\invisible<1-11>{\item[-] Ridge, Elastic-Net, SVM, Random Forests, BART, ...}
\end{itemize}
\invisible<1-12>{\item[-] Which model?  Difficult to know before hand}
\invisible<1-13>{\item[-] Assess out of sample performance with \alert{cross validation}}
\end{itemize}
\invisible<1-14>{\alert{Weighted ensemble}: weights determined by (unique) out of sample predictive performance}

\pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause
\end{frame}


\begin{frame}


Committee Methods:\\

Fit many methods, average with equal weights

\begin{itemize}
\item[-] Voting (classification)
\item[-] Averaging (predictions)
\end{itemize}


Problem: many poor methods may overwhelm high quality fit (remember earlier figures)\\
Solution: learn weights via cross validation


\end{frame}




\begin{frame}
\frametitle{Weighted Ensemble to Classify Documents}

\begin{itemize}
\item[-] Suppose we have $M$ $(m = 1, \hdots, M)$ models.
\end{itemize}
\begin{eqnarray}
\invisible<1>{\alert<14->{\text{Pr}(Y_{i}   =  \text{C}_{1} | \textbf{x}\only<1-13>{_{i}}\only<14->{_{i,\text{test}}} )_{\text{train}} } & = & \alert<14->{  \sum_{m=1}^{M} \alert<3>{\widehat{\pi}_{m} } \alert<4>{\widehat{g}_{m}(\textbf{x}\only<1-13>{_{i}}\only<14->{_{i, \text{test}}} )}} \nonumber }
\end{eqnarray}

\begin{itemize}
\invisible<1-4>{\item[-] Estimate weights $(\widehat{\pi}_{m})$}
\only<1-11>{
\begin{itemize}
\invisible<1-5>{\item[-] K-fold cross validation: generate $M$ out of sample predictions for each document in training set}
\invisible<1-6>{\item[] $\widehat{\textbf{Y}}_{i}  = (\widehat{Y}_{i1}, \widehat{Y}_{i2}, \hdots, \widehat{Y}_{iM} )$}
\invisible<1-7>{\item[-] Estimate weights with constrained regression:}
\begin{eqnarray}
\invisible<1-8>{Y_{i} & = & \sum_{m=1}^{M} \pi_{m} \hat{Y}_{im} + \epsilon_{i} \nonumber }
\end{eqnarray}
\invisible<1-9>{\item[] where we impose constraints: $\pi_{m} \geq 0 $ and $\sum_{m=1}^{M} \pi_{m} = 1$. }
\invisible<1-10>{\item[-] Result $\widehat{\pi}_{m}$ for each method }
\end{itemize}
}
\invisible<1-11>{\item[-] Estimate  $\widehat{g}_{m}(\textbf{x}_{i} ) \leadsto $ Apply all $M$ models to entire training set}
\end{itemize}
\invisible<1-12>{3) For each document $i$ in test set, $\textbf{x}_{i, \text{test}}$ } \\
%\begin{eqnarray}
%\invisible<1-11>{\text{Pr}(Y_{i} = \text{Credit} | \textbf{w}_{i} ) & = & \sum_{m=1}^{M} \widehat{\pi}_{m} \widehat{g}_{m}(\textbf{w}_{i} ) \nonumber }
%\end{eqnarray}
\invisible<1-14>{(Classify if above threshold)} \\


\pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause \pause
\end{frame}




\begin{frame}


\scalebox{0.6}{\includegraphics{SuperLearner.png}}


\end{frame}



\begin{frame}
\frametitle{Why Super Learn?}

van der Laan et al (2007) prove:
\begin{itemize}
\item[-] \alert{Asymptotically}: super learners will perform as well the \alert{best} candidates for data
\item[-] \alert{Oracle}: performs like the best possible method among candidate methods
\begin{itemize}
\item[-] Asymptotically outperforms constituent methods
\item[-] Performs as well as optimal combinations of those methods
\end{itemize}
\end{itemize}

Practical questions:
\begin{itemize}
\item[-] Final regression:
\begin{itemize}
\item[-] Logistic
\item[-] Linear
\item[-] \alert{Could super learn again!}
\end{itemize}
\item[-] How Many Folds?
\begin{itemize}
\item[-] van der Laan et al's proofs rely on growing folds with $N$ (but slowly)
\item[-] Use 10-fold cross validation for simulations
\end{itemize}
\end{itemize}


\end{frame}



\begin{frame}
\frametitle{Impression of Influence}

Estimate: $Y_{i} \in \{\text{Credit}, \text{Not Credit} \}$ \pause \\
\begin{itemize}
\invisible<1>{\item[-] Triple hand code 800 press releases} \pause
\invisible<1-2>{\item[-] Resolve disagreement with voting$\leadsto$ few disagreements} \pause
\end{itemize}

\invisible<1-3>{Use five classifiers to form Ensemble (cross validating within each to tune parameters)}\pause
\begin{itemize}
\invisible<1-4>{\item[-] LASSO} \pause \invisible<1-9>{0}
\invisible<1-5>{\item[-] Elastic-Net}\pause \invisible<1-9>{ 23\%}
\invisible<1-6>{\item[-] Random Forest} \pause \invisible<1-9>{61\%}
\invisible<1-7>{\item[-] A Support Vector Machine}\pause \invisible<1-9>{16\%}
\invisible<1-8>{\item[-] Kernel Regularized Least Squares (KRLS, Hainmueller and Hazlett 2014)}\pause \invisible<1-9>{ 0}
\end{itemize}





\end{frame}



\begin{frame}
\frametitle{Strategic Credit Claiming to Build a Personal Vote}

\begin{tikzpicture}
\node (dummy1) at (-8, 8) [] {} ;
\node at (-6, 8) [] {\scalebox{0.4}{\includegraphics{DensTikz.pdf}}};

\invisible<1>{\node (trial) at (-8.75,5.7) [] {} ;
\only<1-9>{\node(Burton) at (-10, 10) [] {\scalebox{0.5}{\includegraphics{Burton.jpg}}};
\draw[->, line width = 1.5pt] (Burton) to [out = 270, in = 90] (trial) ; }

\only<3>{\node (McGroff) at (-3, 11) [] {\alert{John McGroff}: ``voted for every spending bill "} ; }
\only<3>{\node (McGroff2) at (-3, 10.6) [] {that went through the office"} ;}
\only<4>{\node (McGroff3) at (-3, 11) [] {\alert{John McGroff}:  ``Not the actions of a fiscally "};}
\only<4>{\node (McGroff4) at (-3, 10.6) [] {conservative congressman who } ; }
\only<4>{\node (McGroff5) at (-3, 10.2) [] {cares about personal responsibility"}; }
}
%\invisible<1-4>{\node (Price) at (-10, 8) [] {\scalebox{0.2}{\includegraphics{Price.jpg}}} ;
%\node (trial1) at (-8.7,5.7) [] {} ;
%\draw[->, line width = 1.5pt] (Price) to [out = 0, in = 90] (trial1) ; }

\only<1-9>{
\invisible<1-4>{\node (Flake) at (-8.2, 11) [] {\scalebox{0.1}{\includegraphics{Flake.jpg}}} ;
\node (trial2) at (-8.6,5.7) [] {} ;
\draw[->, line width = 1.5pt] (Flake) to [out = 270, in = 90] (trial2) ; }


\invisible<1-5>{\node (Degette) at (-10, 6) [] {\scalebox{0.1}{\includegraphics{Jan.jpg}}} ;
\node (trial3) at (-8.65, 5.7) [] {} ;
\draw[->, line width = 1.5pt] (Degette) to [out = 30, in = 90] (trial2) ; }

\invisible<1-6>{\node (Lobiondo) at (-6, 10) [] {\scalebox{0.08}{\includegraphics{Lobiondo.jpg}}} ;
\node (trial4) at (-5.1, 5.7) [] {} ;
\draw[->, line width = 1.5pt] (Lobiondo) to [out = 270, in = 90] (trial4) ; }

\invisible<1-7>{\node (Lobiondo) at (-4, 9.5) [] {\scalebox{0.125}{\includegraphics{Edwards.jpg}}} ;
\node (trial5) at (-4.95, 5.7) [] {} ;
\draw[->, line width = 1.5pt] (Lobiondo) to [out = 270, in = 90] (trial5) ; }}

\invisible<1-8>{\node (Rogers) at (-1.8, 11) [] {\scalebox{0.15}{\includegraphics{Rogers.jpg}}};
\node (trial6) at (-2.4, 5.7) [] {} ;
\draw[->, line width = 1.5pt] (Rogers) to [out = 270, in = 90] (trial6) ; }

\only<10>{\node (Pork1) at (-7, 11)  [] {``We just can't afford luxuries like ideology" } ; }
\only<11>{\node (Pork2) at (-7, 11)  [] {Lexington Herald-Leader: \alert{Prince of Pork}} ; }


\end{tikzpicture}

\pause \pause \pause \pause \pause \pause \pause \pause \pause \pause

%\only<1>{\scalebox{0.4}{\includegraphics{DensTikz.pdf}}}
%\only<1>{\scalebox{0.4}{\includegraphics{NewDensity1.pdf}}}\only<2>{\scalebox{0.4}{\includegraphics{NewDensity2.pdf}}}\only<3>{\scalebox{0.4}{\includegraphics{NewDensity3.pdf}}}\only<4>{\scalebox{0.4}{\includegraphics{NewDensity4.pdf}}}\only<5>{\scalebox{0.4}{\includegraphics{NewDensity5.pdf}}}\only<6>{\scalebox{0.4}{\includegraphics{NewDensity9.pdf}}}\only<7>{\scalebox{0.4}{\includegraphics{NewDensity10.pdf}}}\only<8>{\scalebox{0.4}{\includegraphics{NewDensity11.pdf}}}\only<9>{\scalebox{0.4}{\includegraphics{NewDensity12.pdf}}}\only<10>{\scalebox{0.4}{\includegraphics{NewDensity13.pdf}}}


\end{frame}





\begin{frame}
\frametitle{Other Reasons to Ensemble (Dietterich 2000) }

Statistical \pause
\begin{itemize}
\invisible<1>{\item[-] With little data, many algorithms offer similar performance } \pause
\invisible<1-2>{\item[-] Ensemble ensures we avoid \alert{wrong} model in test set } \pause
\end{itemize}
\invisible<1-3>{Computational } \pause
\begin{itemize}
\invisible<1-4>{\item[-] Methods stuck in local modes} \pause
\invisible<1-5>{\item[-] Result: no one run provides best model} \pause
\invisible<1-6>{\item[-] Averages of runs may perform better } \pause
\end{itemize}
\invisible<1-7>{Complex ``true" functional forms } \pause
\begin{itemize}
\invisible<1-8>{\item[-] One method may be unable to approximate true DGP } \pause
\invisible<1-9>{\item[-] Mixtures of methods may approximate better } \pause
\end{itemize}


\end{frame}



\end{document}